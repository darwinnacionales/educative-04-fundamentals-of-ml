{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f4d943c",
   "metadata": {},
   "source": [
    "# `mnist.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3959442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An MNIST loader.\n",
    "\n",
    "import numpy as np\n",
    "import gzip\n",
    "import struct\n",
    "\n",
    "\n",
    "def load_images(filename):\n",
    "    # Open and unzip the file of images:\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        # Read the header information into a bunch of variables:\n",
    "        _ignored, n_images, columns, rows = struct.unpack('>IIII', f.read(16))\n",
    "        # Read all the pixels into a NumPy array of bytes:\n",
    "        all_pixels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        # Reshape the pixels into a matrix where each line is an image:\n",
    "        return all_pixels.reshape(n_images, columns * rows)\n",
    "\n",
    "\n",
    "def prepend_bias(X):\n",
    "    # Insert a column of 1s in the position 0 of X.\n",
    "    # (“axis=1” stands for: “insert a column, not a row”)\n",
    "    return np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "\n",
    "# 60000 images, each 785 elements (1 bias + 28 * 28 pixels)\n",
    "X_train = prepend_bias(load_images(\"input/train-images.idx3-ubyte.gz\"))\n",
    "\n",
    "# 10000 images, each 785 elements, with the same structure as X_train\n",
    "X_test = prepend_bias(load_images(\"input/t10k-images.idx3-ubyte.gz\"))\n",
    "\n",
    "\n",
    "def load_labels(filename):\n",
    "    # Open and unzip the file of images:\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        # Skip the header bytes:\n",
    "        f.read(8)\n",
    "        # Read all the labels into a list:\n",
    "        all_labels = f.read()\n",
    "        # Reshape the list of labels into a one-column matrix:\n",
    "        return np.frombuffer(all_labels, dtype=np.uint8).reshape(-1, 1)\n",
    "\n",
    "\n",
    "def one_hot_encode(Y):\n",
    "    n_labels = Y.shape[0]\n",
    "    n_classes = 10\n",
    "    encoded_Y = np.zeros((n_labels, n_classes))\n",
    "    for i in range(n_labels):\n",
    "        label = Y[i]\n",
    "        encoded_Y[i][label] = 1\n",
    "    return encoded_Y\n",
    "\n",
    "\n",
    "# 60K labels, each a single digit from 0 to 9\n",
    "Y_train_unencoded = load_labels(\"input/train-labels.idx1-ubyte.gz\")\n",
    "\n",
    "# 60K labels, each consisting of 10 one-hot encoded elements\n",
    "Y_train = one_hot_encode(Y_train_unencoded)\n",
    "\n",
    "# 10000 labels, each a single digit from 0 to 9\n",
    "Y_test = load_labels(\"input/t10k-labels.idx1-ubyte.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1cd841",
   "metadata": {},
   "source": [
    "# `main.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef32e0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - Loss: 6.93147180559945397249, 9.80%\n",
      "20 - Loss: 1.25378277537179583234, 86.22%\n",
      "40 - Loss: 1.08333180073001211774, 87.81%\n",
      "60 - Loss: 1.00734656458890392550, 88.66%\n",
      "80 - Loss: 0.96254470749127318818, 89.22%\n",
      "100 - Loss: 0.93231667470232026940, 89.39%\n",
      "120 - Loss: 0.91020541845761471222, 89.65%\n",
      "140 - Loss: 0.89313863437623119967, 89.80%\n",
      "160 - Loss: 0.87945224017547751760, 90.02%\n",
      "180 - Loss: 0.86815894847049102090, 90.18%\n",
      "199 - Loss: 0.85907344837140864335, 90.32%\n",
      "200 - Loss: 0.85863196488041293453, 90.32%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Applying Logistic Regression\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Basically doing prediction but named forward as its \n",
    "# performing Forward-Propagation\n",
    "def forward(X, w):\n",
    "    weighted_sum = np.matmul(X, w)\n",
    "    return sigmoid(weighted_sum)\n",
    "\n",
    "# Calling the predict() function\n",
    "def classify(X, w):\n",
    "    y_hat = forward(X, w)\n",
    "    labels = np.argmax(y_hat, axis=1)\n",
    "    return labels.reshape(-1, 1)\n",
    "\n",
    "# Computing Loss over using logistic regression\n",
    "def loss(X, Y, w):\n",
    "    y_hat = forward(X, w)\n",
    "    first_term = Y * np.log(y_hat)\n",
    "    second_term = (1 - Y) * np.log(1 - y_hat)\n",
    "    return -np.sum(first_term + second_term) / X.shape[0]\n",
    "\n",
    "# calculating gradient\n",
    "def gradient(X, Y, w):\n",
    "    return np.matmul(X.T, (forward(X, w) - Y)) / X.shape[0]\n",
    "\n",
    "# Printing results to the terminal screen\n",
    "def report(iteration, X_train, Y_train, X_test, Y_test, w):\n",
    "    matches = np.count_nonzero(classify(X_test, w) == Y_test)\n",
    "    n_test_examples = Y_test.shape[0]\n",
    "    matches = matches * 100.0 / n_test_examples\n",
    "    training_loss = loss(X_train, Y_train, w)\n",
    "    if (iteration%20 == 0) or iteration == 199:\n",
    "        print(\"%d - Loss: %.20f, %.2f%%\" % (iteration, training_loss, matches))\n",
    "\n",
    "# calling the training function for desired no. of iterations\n",
    "def train(X_train, Y_train, X_test, Y_test, iterations, lr):\n",
    "    w = np.zeros((X_train.shape[1], Y_train.shape[1]))\n",
    "    for i in range(iterations):\n",
    "        report(i, X_train, Y_train, X_test, Y_test, w)\n",
    "        w -= gradient(X_train, Y_train, w) * lr\n",
    "    report(iterations, X_train, Y_train, X_test, Y_test, w)\n",
    "    return w\n",
    "\n",
    "w = train(X_train, Y_train,\n",
    "          X_test, Y_test,\n",
    "          iterations=200, lr=1e-5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
