{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8758737b",
   "metadata": {},
   "source": [
    "# `mnist.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9259c038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An MNIST loader.\n",
    "\n",
    "import numpy as np\n",
    "import gzip\n",
    "import struct\n",
    "\n",
    "\n",
    "def load_images(filename):\n",
    "    # Open and unzip the file of images:\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        # Read the header information into a bunch of variables\n",
    "        _ignored, n_images, columns, rows = struct.unpack('>IIII', f.read(16))\n",
    "        # Read all the pixels into a NumPy array of bytes:\n",
    "        all_pixels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        # Reshape the pixels into a matrix where each line is an image:\n",
    "        return all_pixels.reshape(n_images, columns * rows)\n",
    "\n",
    "\n",
    "def prepend_bias(X):\n",
    "    # Insert a column of 1s in the position 0 of X.\n",
    "    # (“axis=1” stands for: “insert a column, not a row”)\n",
    "    return np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "\n",
    "# 60000 images, each 785 elements (1 bias + 28 * 28 pixels)\n",
    "X_train = prepend_bias(load_images(\"input/train-images.idx3-ubyte.gz\"))\n",
    "\n",
    "# 10000 images, each 785 elements, with the same structure as X_train\n",
    "X_test = prepend_bias(load_images(\"input/t10k-images.idx3-ubyte.gz\"))\n",
    "\n",
    "\n",
    "def load_labels(filename):\n",
    "    # Open and unzip the file of images:\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        # Skip the header bytes:\n",
    "        f.read(8)\n",
    "        # Read all the labels into a list:\n",
    "        all_labels = f.read()\n",
    "        # Reshape the list of labels into a one-column matrix:\n",
    "        return np.frombuffer(all_labels, dtype=np.uint8).reshape(-1, 1)\n",
    "\n",
    "\n",
    "def encode_digit(Y, digit):\n",
    "    encoded_Y = np.zeros_like(Y)\n",
    "    n_labels = Y.shape[0]\n",
    "    for i in range(n_labels):\n",
    "        if Y[i] == digit:\n",
    "            encoded_Y[i][0] = 1\n",
    "    return encoded_Y\n",
    "\n",
    "\n",
    "TRAINING_LABELS = load_labels(\"input/train-labels.idx1-ubyte.gz\")\n",
    "TEST_LABELS = load_labels(\"input/t10k-labels.idx1-ubyte.gz\")\n",
    "\n",
    "Y_train = []\n",
    "Y_test = []\n",
    "\n",
    "for digit in range(10):\n",
    "    Y_train.append(encode_digit(TRAINING_LABELS, digit))\n",
    "    Y_test.append(encode_digit(TEST_LABELS, digit))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c71b4d",
   "metadata": {},
   "source": [
    "# `main.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "617b8021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0 => Loss: 0.69314718055994528623\n",
      "Iteration   50 => Loss: 0.05717257769170278059\n",
      "Iteration   99 => Loss: 0.04818443983574007689\n",
      "Correct classifications for digit 0: 9899/10000 (98.99%)\n",
      "Iteration    0 => Loss: 0.69314718055994528623\n",
      "Iteration   50 => Loss: 0.04907664716890068612\n",
      "Iteration   99 => Loss: 0.04270086047894513376\n",
      "Correct classifications for digit 1: 9903/10000 (99.03%)\n",
      "Iteration    0 => Loss: 0.69314718055994528623\n",
      "Iteration   50 => Loss: 0.10590272802562278320\n",
      "Iteration   99 => Loss: 0.09450611475477797840\n",
      "Correct classifications for digit 2: 9737/10000 (97.37%)\n",
      "Iteration    0 => Loss: 0.69314718055994528623\n",
      "Iteration   50 => Loss: 0.11989407489302174314\n",
      "Iteration   99 => Loss: 0.10998321696402391101\n",
      "Correct classifications for digit 3: 9698/10000 (96.98%)\n",
      "Iteration    0 => Loss: 0.69314718055994528623\n",
      "Iteration   50 => Loss: 0.08956748403271110048\n",
      "Iteration   99 => Loss: 0.07723638000157202754\n",
      "Correct classifications for digit 4: 9759/10000 (97.59%)\n",
      "Iteration    0 => Loss: 0.69314718055994528623\n",
      "Iteration   50 => Loss: 0.13322443624926735839\n",
      "Iteration   99 => Loss: 0.11895658384798543650\n",
      "Correct classifications for digit 5: 9637/10000 (96.37%)\n",
      "Iteration    0 => Loss: 0.69314718055994528623\n",
      "Iteration   50 => Loss: 0.06870863595608994734\n",
      "Iteration   99 => Loss: 0.05940363515304513536\n",
      "Correct classifications for digit 6: 9807/10000 (98.07%)\n",
      "Iteration    0 => Loss: 0.69314718055994528623\n",
      "Iteration   50 => Loss: 0.07362104540788157181\n",
      "Iteration   99 => Loss: 0.06515325905362676084\n",
      "Correct classifications for digit 7: 9814/10000 (98.14%)\n",
      "Iteration    0 => Loss: 0.69314718055994528623\n",
      "Iteration   50 => Loss: 0.19340882391152630637\n",
      "Iteration   99 => Loss: 0.18218087631610888066\n",
      "Correct classifications for digit 8: 9385/10000 (93.85%)\n",
      "Iteration    0 => Loss: 0.69314718055994528623\n",
      "Iteration   50 => Loss: 0.14896239910344186153\n",
      "Iteration   99 => Loss: 0.13529509731233321790\n",
      "Correct classifications for digit 9: 9557/10000 (95.57%)\n"
     ]
    }
   ],
   "source": [
    "# A binary classifier that recognizes one of the digits in MNIST.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Applying Logistic Regression\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Basically doing prediction but named forward as its \n",
    "# performing Forward-Propagation\n",
    "def forward(X, w):\n",
    "    weighted_sum = np.matmul(X, w)\n",
    "    return sigmoid(weighted_sum)\n",
    "\n",
    "# Calling the predict() function\n",
    "def classify(X, w):\n",
    "    return np.round(forward(X, w))\n",
    "\n",
    "\n",
    "# Computing Loss over using logistic regression\n",
    "def loss(X, Y, w):\n",
    "    y_hat = forward(X, w)\n",
    "    first_term = Y * np.log(y_hat)\n",
    "    second_term = (1 - Y) * np.log(1 - y_hat)\n",
    "    return -np.average(first_term + second_term)\n",
    "\n",
    "\n",
    "# calculating gradient\n",
    "def gradient(X, Y, w):\n",
    "    return np.matmul(X.T, (forward(X, w) - Y)) / X.shape[0]\n",
    "\n",
    "# calling the training function for desired no. of iterations\n",
    "def train(X, Y, iterations, lr):\n",
    "    w = np.zeros((X.shape[1], 1))\n",
    "    for i in range(iterations):\n",
    "        if i == 0 or i == 50 or i == iterations - 1:\n",
    "            print('Iteration %4d => Loss: %.20f' % (i, loss(X, Y, w)))\n",
    "        w -= gradient(X, Y, w) * lr\n",
    "    return w\n",
    "\n",
    "# Doing inference to test our model\n",
    "def test(X, Y, w, digit):\n",
    "    total_examples = X.shape[0]\n",
    "    correct_results = np.sum(classify(X, w) == Y)\n",
    "    success_percent = correct_results * 100 / total_examples\n",
    "    print(\"Correct classifications for digit %d: %d/%d (%.2f%%)\" %\n",
    "          (digit, correct_results, total_examples, success_percent))\n",
    "\n",
    "for digit in range(10):\n",
    "    w = train(X_train, Y_train[digit], iterations=100, lr=1e-5)\n",
    "    test(X_test, Y_test[digit], w, digit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8319b742",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
